{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      " loss Training 6.0475715753868595\n",
      "Accuracy 6.2439024390243905\n",
      "\n",
      "\n",
      "epoch 2\n",
      " loss Training 5.414247521905353\n",
      "Accuracy 5.8341463414634145\n",
      "\n",
      "\n",
      "epoch 3\n",
      " loss Training 4.878615767576241\n",
      "Accuracy 5.5024390243902435\n",
      "\n",
      "\n",
      "epoch 4\n",
      " loss Training 4.44556526013357\n",
      "Accuracy 5.170731707317073\n",
      "\n",
      "\n",
      "epoch 5\n",
      " loss Training 4.098833194029449\n",
      "Accuracy 5.11219512195122\n",
      "\n",
      "\n",
      "epoch 6\n",
      " loss Training 3.8136946420286955\n",
      "Accuracy 6.126829268292683\n",
      "\n",
      "\n",
      "epoch 7\n",
      " loss Training 3.5727774340866136\n",
      "Accuracy 8.175609756097561\n",
      "\n",
      "\n",
      "epoch 8\n",
      " loss Training 3.3664486121815553\n",
      "Accuracy 11.121951219512194\n",
      "\n",
      "\n",
      "epoch 9\n",
      " loss Training 3.1887811404768445\n",
      "Accuracy 13.365853658536585\n",
      "\n",
      "\n",
      "epoch 10\n",
      " loss Training 3.0355559976884585\n",
      "Accuracy 14.263414634146342\n",
      "\n",
      "\n",
      "epoch 11\n",
      " loss Training 2.9036325899348907\n",
      "Accuracy 14.009756097560976\n",
      "\n",
      "\n",
      "epoch 12\n",
      " loss Training 2.790695203372614\n",
      "Accuracy 13.502439024390243\n",
      "\n",
      "\n",
      "epoch 13\n",
      " loss Training 2.6950641908382837\n",
      "Accuracy 12.64390243902439\n",
      "\n",
      "\n",
      "epoch 14\n",
      " loss Training 2.6154398286635745\n",
      "Accuracy 13.580487804878048\n",
      "\n",
      "\n",
      "epoch 15\n",
      " loss Training 2.550446392921425\n",
      "Accuracy 17.151219512195123\n",
      "\n",
      "\n",
      "epoch 16\n",
      " loss Training 2.4981458997709436\n",
      "Accuracy 20.8\n",
      "\n",
      "\n",
      "epoch 17\n",
      " loss Training 2.456003912608917\n",
      "Accuracy 22.985365853658536\n",
      "\n",
      "\n",
      "epoch 18\n",
      " loss Training 2.4213944616436978\n",
      "Accuracy 23.68780487804878\n",
      "\n",
      "\n",
      "epoch 19\n",
      " loss Training 2.3921306543301717\n",
      "Accuracy 24.17560975609756\n",
      "\n",
      "\n",
      "epoch 20\n",
      " loss Training 2.3666475678369348\n",
      "Accuracy 24.565853658536586\n",
      "\n",
      "\n",
      "epoch 21\n",
      " loss Training 2.3439129230252966\n",
      "Accuracy 24.917073170731708\n",
      "\n",
      "\n",
      "epoch 22\n",
      " loss Training 2.3232625374432216\n",
      "Accuracy 25.13170731707317\n",
      "\n",
      "\n",
      "epoch 23\n",
      " loss Training 2.3042628319190497\n",
      "Accuracy 25.26829268292683\n",
      "\n",
      "\n",
      "epoch 24\n",
      " loss Training 2.2866197845788068\n",
      "Accuracy 25.307317073170733\n",
      "\n",
      "\n",
      "epoch 25\n",
      " loss Training 2.27012418554637\n",
      "Accuracy 25.346341463414635\n",
      "\n",
      "\n",
      "epoch 26\n",
      " loss Training 2.2546198814713216\n",
      "Accuracy 25.482926829268294\n",
      "\n",
      "\n",
      "epoch 27\n",
      " loss Training 2.2399853916056145\n",
      "Accuracy 25.5609756097561\n",
      "\n",
      "\n",
      "epoch 28\n",
      " loss Training 2.2261230450197034\n",
      "Accuracy 25.5609756097561\n",
      "\n",
      "\n",
      "epoch 29\n",
      " loss Training 2.2129523251542618\n",
      "Accuracy 25.6\n",
      "\n",
      "\n",
      "epoch 30\n",
      " loss Training 2.200405599751809\n",
      "Accuracy 25.658536585365855\n",
      "\n",
      "\n",
      "epoch 31\n",
      " loss Training 2.188425241943721\n",
      "Accuracy 25.73658536585366\n",
      "\n",
      "\n",
      "epoch 32\n",
      " loss Training 2.176961595801652\n",
      "Accuracy 25.775609756097563\n",
      "\n",
      "\n",
      "epoch 33\n",
      " loss Training 2.165971479575199\n",
      "Accuracy 25.814634146341465\n",
      "\n",
      "\n",
      "epoch 34\n",
      " loss Training 2.155417048845128\n",
      "Accuracy 25.814634146341465\n",
      "\n",
      "\n",
      "epoch 35\n",
      " loss Training 2.145264912091816\n",
      "Accuracy 25.931707317073172\n",
      "\n",
      "\n",
      "epoch 36\n",
      " loss Training 2.1354854303442576\n",
      "Accuracy 25.990243902439026\n",
      "\n",
      "\n",
      "epoch 37\n",
      " loss Training 2.126052155158313\n",
      "Accuracy 26.048780487804876\n",
      "\n",
      "\n",
      "epoch 38\n",
      " loss Training 2.116941372789721\n",
      "Accuracy 26.12682926829268\n",
      "\n",
      "\n",
      "epoch 39\n",
      " loss Training 2.108131731090186\n",
      "Accuracy 26.10731707317073\n",
      "\n",
      "\n",
      "epoch 40\n",
      " loss Training 2.099603931483176\n",
      "Accuracy 26.146341463414632\n",
      "\n",
      "\n",
      "epoch 41\n",
      " loss Training 2.0913404725065323\n",
      "Accuracy 26.146341463414632\n",
      "\n",
      "\n",
      "epoch 42\n",
      " loss Training 2.083325434460571\n",
      "Accuracy 26.165853658536584\n",
      "\n",
      "\n",
      "epoch 43\n",
      " loss Training 2.0755442970197873\n",
      "Accuracy 26.24390243902439\n",
      "\n",
      "\n",
      "epoch 44\n",
      " loss Training 2.0679837834597667\n",
      "Accuracy 26.321951219512194\n",
      "\n",
      "\n",
      "epoch 45\n",
      " loss Training 2.0606317265457523\n",
      "Accuracy 26.4\n",
      "\n",
      "\n",
      "epoch 46\n",
      " loss Training 2.0534769522154432\n",
      "Accuracy 26.380487804878047\n",
      "\n",
      "\n",
      "epoch 47\n",
      " loss Training 2.04650917802984\n",
      "Accuracy 26.41951219512195\n",
      "\n",
      "\n",
      "epoch 48\n",
      " loss Training 2.039718924014527\n",
      "Accuracy 26.497560975609755\n",
      "\n",
      "\n",
      "epoch 49\n",
      " loss Training 2.0330974340087233\n",
      "Accuracy 26.517073170731706\n",
      "\n",
      "\n",
      "epoch 50\n",
      " loss Training 2.0266366060175103\n",
      "Accuracy 26.59512195121951\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import math\n",
    "\n",
    "# def shuffle(y1, y2):\n",
    "#     p = np.random.permutation(y1.shape[0])\n",
    "#     return y1[p], y2[p]\n",
    "\n",
    "DATA_PINES= scipy.io.loadmat(r\"C:\\Users\\Ayush\\Downloads\\Indian_pines_corrected.mat\")\n",
    "Ground_table_pines = scipy.io.loadmat(r\"C:\\Users\\Ayush\\Downloads\\Indian_pines_gt.mat\")\n",
    "learning_rate = 0.5\n",
    "lmd= 0.0001\n",
    "DATA = DATA_PINES[\"indian_pines_corrected\"]\n",
    "Ground_truth = Ground_table_pines[\"indian_pines_gt\"]\n",
    "\n",
    "\n",
    "##########################reshaping  data Of size (145*145*200) to (21025*2)#################################################\n",
    "DATA = DATA.reshape(21025, 200)\n",
    "DATA= DATA.astype(np.float64)\n",
    "Ground_truth = Ground_truth.reshape(21025)\n",
    "\n",
    "##############Removing all elements of class 0, i.e, with 0 value in the ground truth table################################## \n",
    "indices = np.nonzero(Ground_truth)\n",
    "Ground_truth = Ground_truth[indices]\n",
    "DATA = DATA[indices]\n",
    "Ground_truth = np.subtract(Ground_truth, 1)\n",
    "\n",
    "###########################################Converting into one hot encoded###################################################\n",
    "OneHotGt = np.zeros((Ground_truth.shape[0],16))\n",
    "for i in range(0, Ground_truth.shape[0]):\n",
    "    OneHotGt[i][Ground_truth[i]] = 1\n",
    "\n",
    "#################################################scaling of feature###########################################################\n",
    "for f in range(0, DATA.shape[1]):\n",
    "    mean = np.mean(DATA[:,f])\n",
    "    minimum = np.min(DATA[:,f])\n",
    "    maximum = np.max(DATA[:,f])\n",
    "    DATA[:,f] = (DATA[:,f]-mean)/(maximum- minimum)\n",
    "\n",
    "\n",
    "#########################################Splitting in train and test DATA_PINES################################################\n",
    "train_ground = Ground_truth[0:int(Ground_truth.shape[0]*0.5)]\n",
    "test_ground = Ground_truth[int(Ground_truth.shape[0]*0.5):Ground_truth.shape[0]]\n",
    "train_DATA_PINES= DATA[0:int(DATA.shape[0]*0.5)]\n",
    "test_DATA_PINES= DATA[int(DATA.shape[0]*0.5):DATA.shape[0]]\n",
    "train_Ground_table_pines = OneHotGt[0:int(Ground_truth.shape[0]*0.5)]\n",
    "test_get = OneHotGt[int(Ground_truth.shape[0]*0.5):Ground_truth.shape[0]]\n",
    "\n",
    "################################################randomly initialising weights##################################################\n",
    "weights = np.random.randn(16,200)\n",
    "\n",
    "################################################# doing gradient descent #######################################################\n",
    "for epoch in range(0, 60000):\n",
    "    print(\"epoch\", epoch+1)\n",
    "    M = np.matmul(train_DATA_PINES, np.transpose(weights))\n",
    "    exp_M = np.exp(M)\n",
    "    Loss = 0\n",
    "########################### calculating loss by summing for each class for each training example ##############################\n",
    "    for i in range(0, exp_M.shape[0]):\n",
    "        sum = (np.sum(exp_M[i,:]))\n",
    "        j = train_ground[i]\n",
    "        Loss= Loss+ np.log((exp_M[i][j]/sum))\n",
    "\n",
    "    Loss = (-Loss/train_DATA_PINES.shape[0]) + lmd*np.sum(np.sum(np.matmul(weights, np.transpose(weights))))\n",
    "    print(\" loss Training\", Loss)\n",
    "    probability = np.zeros((train_DATA_PINES.shape[0],16))\n",
    "    for i in range(0, train_DATA_PINES.shape[0]):\n",
    "        sum = (np.sum(exp_M[i,:]))\n",
    "        for j in range(0, 16):\n",
    "            probability[i][j] = exp_M[i][j]/sum\n",
    "\n",
    "###############################################Testing accuracy###############################################################\n",
    "    M_test = np.matmul(test_DATA_PINES, np.transpose(weights))\n",
    "    exponential_of_M_test = np.exp(M_test)\n",
    "    probability_test = np.zeros((test_DATA_PINES.shape[0],16))\n",
    "    for i in range(0, test_DATA_PINES.shape[0]):\n",
    "        sum = (np.sum(exponential_of_M_test[i,:]))\n",
    "        for j in range(0, 16):\n",
    "            probability_test[i][j] = exponential_of_M_test[i][j]/sum\n",
    "    predict = np.argmax(probability_test, axis =1)\n",
    "    accuracy = float(np.sum(predict==test_ground)/ test_DATA_PINES.shape[0])\n",
    "    count=0\n",
    "    for a in range(0, predict.shape[0]):\n",
    "        if(predict[a]==test_ground[a]):\n",
    "            count = count+1\n",
    "    print(\"Accuracy\", float(count)*100/test_DATA_PINES.shape[0])\n",
    "\n",
    "\n",
    "    #weight update\n",
    "    grad = np.zeros((16,200))\n",
    "    grad = np.matmul(np.transpose(train_DATA_PINES), train_Ground_table_pines-probability)\n",
    "    weights = weights- (learning_rate*np.transpose(grad)*(-1)/train_DATA_PINES.shape[0]) + lmd*weights\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

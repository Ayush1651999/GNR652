{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59743879 0.79511698 0.78346395 0.8000796  0.79244469 0.79809746\n",
      " 0.79985706 0.79984963 0.79981917 0.79820914 0.79997024 0.79709111\n",
      " 0.79976673 0.79788645 0.79956263 0.79823887 0.80011808]\n",
      "3.836286650218786\n",
      "1.958644084620477\n"
     ]
    }
   ],
   "source": [
    "#Gradient Descent algorithm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "x=pd.read_csv(\"C:/Users/Ayush/Documents/Ayush/Behavior of the urban traffic of the city of Sao Paulo in Brazil.csv\" , delimiter=';', decimal=',')\n",
    "x= x.values\n",
    "\n",
    "#Taking the initial vales of parameters, i.e, theta to be 0.8\n",
    "theta= np.zeros(shape=(17))\n",
    "for i in range(17) :\n",
    "    theta[i] = theta[i] + 0.8  \n",
    "\n",
    "#Defining the cost function for the gradient descent \n",
    "def costFunctionReg(X_2,y_2,theta,lamda = 0.01):    \n",
    "    #Initializations\n",
    "       \n",
    "    m_2 = len(y_2)\n",
    "    \n",
    "    #Computations\n",
    "    h_2 = X_2 @ theta\n",
    "    J_reg = (lamda / (2*m_2)) * np.sum(np.square(theta))\n",
    "    J = float((1./(2*m_2)) * (h_2 - y_2).T @ (h_2 - y_2)) + J_reg\n",
    "    \n",
    "    if np.isnan(J):\n",
    "        return(np.inf)\n",
    "    \n",
    "    return(J) \n",
    "\n",
    "#We have this to minimize least square error on test data, avoiding underfitting and overfitting\n",
    "num_iters = 45\n",
    "\n",
    "#Choosing the training data, this gives minimum error \n",
    "X = x[0:108,0:17]\n",
    "y = x[0:108,17]\n",
    "    \n",
    "for i in range(num_iters):\n",
    "        #Hypothesis function\n",
    "        h = np.dot(X,theta)\n",
    "        m = len(y)\n",
    "        lamda = 0.01\n",
    "        alpha = 0.0001     #Our step size is 0.0001\n",
    "        T = np.transpose(X)\n",
    "        \n",
    "        cf = costFunctionReg(X, y, theta,lamda = 0.01) #Computing the initial cost function \n",
    "\n",
    "        #Calculating the grad function in vectorized form       \n",
    "        grad_term = (1/m)* ((T @ (h-y)) + lamda * theta )\n",
    "        \n",
    "        #main gradient descent algo \n",
    "        theta = theta - alpha * grad_term\n",
    "        \n",
    "        #Choosing the value of theta which gives the minimum cost function \n",
    "        if (costFunctionReg(X, y, theta, lamda = 0.01) < cf):\n",
    "            Parameter = theta\n",
    "\n",
    "#Choosing the test dataset            \n",
    "X_1 = x[108:135,0:17]\n",
    "y_1 = x[108:135,17]\n",
    "\n",
    "print(Parameter)\n",
    "\n",
    "#Calculating the least square error \n",
    "Variance = costFunctionReg(X_1 ,y_1 , Parameter,lamda = 0.01)\n",
    "print(Variance)\n",
    "\n",
    "#Standard deviation will give the correct measure of error in our hypothesis\n",
    "Standard_deviation = math.sqrt(Variance)\n",
    "print(Standard_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.56139736  0.79219955  0.05466581  2.81049766  0.10783567 -0.42805108\n",
      "  1.79244059 -2.20774971 -2.65414468  1.58907402  2.01124812  0.77828344\n",
      "  3.68095407  0.04915012  0.49514329  0.62638158  3.60766378]\n",
      "2.587916541570221\n"
     ]
    }
   ],
   "source": [
    "#Closed form solution\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "x=pd.read_csv(\"C:/Users/Ayush/Documents/Ayush/Behavior of the urban traffic of the city of Sao Paulo in Brazil.csv\" , delimiter=';', decimal=',')\n",
    "x= x.values\n",
    "lamda = 0.01\n",
    "\n",
    "#Choosing the training data\n",
    "X = x[0:108,0:17]\n",
    "y = x[0:108,17]\n",
    "T = np.transpose(X)\n",
    "Id = np.identity(17)\n",
    "\n",
    "#Computations\n",
    "I = np.linalg.inv(T@X + lamda*Id)\n",
    "A = I@T\n",
    "theta = A@y\n",
    "print(theta)\n",
    "\n",
    "#Defining the cost function to calculate error\n",
    "def costFunctionReg(X_2,y_2,theta,lamda = 0.01):    \n",
    "    #Initializations\n",
    "       \n",
    "    m_2 = len(y_2)\n",
    "    \n",
    "    #Computations\n",
    "    h_2 = X_2 @ theta\n",
    "    J_reg = (lamda / (2*m_2)) * np.sum(np.square(theta))\n",
    "    J = float((1./(2*m_2)) * (h_2 - y_2).T @ (h_2 - y_2)) + J_reg;\n",
    "    \n",
    "    if np.isnan(J):\n",
    "        return(np.inf)\n",
    "    \n",
    "    return(J) \n",
    "\n",
    "#Choosing the test data det\n",
    "X_1 = x[108:135,0:17]\n",
    "y_1 = x[108:135,17]\n",
    "\n",
    "#Result time\n",
    "Variance = costFunctionReg(X_1, y_1, theta, lamda = 0.01)\n",
    "\n",
    "Standard_deviation = math.sqrt(Variance)\n",
    "print(Standard_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59743879 0.79511698 0.78346395 0.8000796  0.79244469 0.79809746\n",
      " 0.79985706 0.79984963 0.79981917 0.79820914 0.79997024 0.79709111\n",
      " 0.79976673 0.79788645 0.79956263 0.79823887 0.80011808]\n",
      "3.836286650218786\n",
      "1.958644084620477\n"
     ]
    }
   ],
   "source": [
    "#Maximum Likelihood estimate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "x=pd.read_csv(\"C:/Users/Ayush/Documents/Ayush/Behavior of the urban traffic of the city of Sao Paulo in Brazil.csv\" , delimiter=';', decimal=',')\n",
    "x= x.values\n",
    "\n",
    "#taking the intial value sof parameters \n",
    "theta= np.zeros(shape=(17))\n",
    "for i in range(17) :\n",
    "    theta[i] = theta[i] + 0.8\n",
    "\n",
    "#defining the negative of maximum likelihood function    \n",
    "def negMaxLikeFunc(X_2,y_2,theta,lamda = 0.01):    \n",
    "    #Initializations\n",
    "       \n",
    "    m_2 = len(y_2)\n",
    "    \n",
    "    #Computations\n",
    "    h_2 = X_2 @ theta\n",
    "    J_reg = (lamda / (2*m_2)) * np.sum(np.square(theta))\n",
    "    J =  float((1./(2*m_2)) * (h_2 - y_2).T @ (h_2 - y_2)) + J_reg\n",
    "    \n",
    "    if np.isnan(J):\n",
    "        return(np.inf)\n",
    "    \n",
    "    return(J) \n",
    "\n",
    "num_iters = 45\n",
    "\n",
    "#Seelcting the training dataset\n",
    "X = x[0:108,0:17]\n",
    "y = x[0:108,17]\n",
    "    \n",
    "for i in range(num_iters):\n",
    "        #Hypothesis function\n",
    "        h = np.dot(X,theta)\n",
    "        m = len(y)\n",
    "        lamda = 0.01\n",
    "        alpha = 0.0001   #we have chosen the value of alpha to be 0.0001\n",
    "        T = np.transpose(X)\n",
    "        cf = negMaxLikeFunc(X, y, theta,lamda = 0.01)\n",
    "        \n",
    "        Step_size = (1/m)* ((T @ (h-y)) + lamda * theta )\n",
    "        \n",
    "        #Calculating the grad function in vectorized form\n",
    "        theta = theta - alpha * Step_size\n",
    "        \n",
    "        #Choosing the value of parameter that maximizes the likelihood\n",
    "        #Minimizing the function we defined is same as maximizing the maximum likelihood function \n",
    "        if (negMaxLikeFunc(X, y, theta, lamda = 0.01) < cf):\n",
    "            Parameter = theta\n",
    "\n",
    "#Selecting the test dataset            \n",
    "X_1 = x[108:135,0:17]\n",
    "y_1 = x[108:135,17]\n",
    "\n",
    "print(Parameter)\n",
    "\n",
    "Variance = negMaxLikeFunc(X_1 ,y_1 , Parameter,lamda = 0.01)\n",
    "print(Variance)  # Because maximum square error is (55 - maxLikeFunc)\n",
    "\n",
    "#Standard deviation will give the correct measure of error in our hypothesis\n",
    "Standard_deviation = math.sqrt(Variance)\n",
    "print(Standard_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
